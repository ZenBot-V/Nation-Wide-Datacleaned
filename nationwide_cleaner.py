# -*- coding: utf-8 -*-
"""Copy of Nationwide.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kDxkoR-YQCG4bTRKkTZRGPfRsMp_lLqf
"""

import pandas as pd

# Define the header mapping from Chinese to English
header_mapping = {
    '车架号': 'VIN',
    '姓名': 'Name',
    '身份证': 'ID_Number',
    '性别': 'Gender',
    '手机': 'Mobile_Phone',
    '邮箱': 'Email',
    '省': 'Province',
    '城市': 'City',
    '地址': 'Address',
    '邮编': 'Postal_Code',
    '生日': 'Date_of_Birth',
    '行业': 'Industry',
    '月薪': 'Monthly_Salary',
    '婚姻': 'Marital_Status',
    '教育': 'Education',
    'BRAND': 'Brand',
    '车系': 'Vehicle_Series',
    '车型': 'Vehicle_Model',
    '配置': 'Configuration',
    '颜色': 'Color',
    '发动机号': 'Engine_Number'
}

# Load the CSV file
file_path = '/content/760k-Car-Owners-Nationwide-China-csv-2020.csv'
df = pd.read_csv(file_path)

# Rename the columns using the header mapping
df.rename(columns=header_mapping, inplace=True)

# Save the modified DataFrame to a new CSV file
output_file_path = '/content/nationwide_trans.csv'
df.to_csv(output_file_path, index=False)

# Display the first few rows to confirm changes
print(df.head())

import pandas as pd

# Load the previously translated CSV file with specified data types
file_path = '/content/nationwide_trans.csv'

# Specifying dtype to avoid mixed types warnings
dtype_mapping = {
    'VIN': str,
    'Name': str,
    'ID_Number': str,
    'Gender': str,
    'Mobile_Phone': str,
    'Email': str,
    'Province': str,
    'City': str,
    'Address': str,
    'Postal_Code': str,
    'Monthly_Salary': str,
    'Marital_Status': str,
    'Education': str,
    'Brand': str,
    'Vehicle_Series': str,
    'Vehicle_Model': str,
    'Configuration': str,
    'Color': str,
    'Engine_Number': str
}

# Load the DataFrame with the specified dtype
df = pd.read_csv(file_path, dtype=dtype_mapping)

# Display the original DataFrame to check the structure
print("Original DataFrame:")
print(df.head())

# Convert potential float values to string before applying .join
for col in ['Address', 'City', 'Province', 'Postal_Code']:
    df[col] = df[col].astype(str)  # Convert any numeric values to strings

# Combine 'Address', 'City', 'Province', 'Postal_Code' into one column 'Full_Address'
df['Full_Address'] = df[['Address', 'City', 'Province', 'Postal_Code']].agg(', '.join, axis=1)

# List of columns to remove
columns_to_remove = [
    'ID_Number',
    'Gender',
    'Address',
    'City',
    'Province',
    'Postal_Code',
    'Date_of_Birth',  # Ensure this matches the translated name
    'Industry',       # Ensure this matches the translated name
    'Monthly_Salary',
    'Marital_Status',
    'Education',
    'Configuration',
    'Color',
    'unnamed_21'      # Trying to remove unnamed_21
]

# Check for 'Unnamed: 21' or similar variations and add them to the list
columns_to_remove += [col for col in df.columns if 'Unnamed' in col]

# Drop only columns that exist in the DataFrame
df_cleaned = df.drop(columns=[col for col in columns_to_remove if col in df.columns])

# Remove duplicate rows
df_cleaned = df_cleaned.drop_duplicates()

# Display the cleaned DataFrame to verify removal and combination
print("\nCleaned DataFrame with combined address and duplicates removed:")
print(df_cleaned[['Full_Address']].head())

# Save the cleaned DataFrame to a new CSV file as nationwide_pan1
output_file_path = '/content/nationwide_pan1.csv'
df_cleaned.to_csv(output_file_path, index=False)

# Verify the columns in the cleaned DataFrame
print("\nColumns in cleaned DataFrame:")
print(df_cleaned.columns)

import pandas as pd

# Load the cleaned CSV file
output_file_path = '/content/nationwide_pan1.csv'
df_validated = pd.read_csv(output_file_path)

# Validation 1: Check for any 'Unnamed' columns (i.e., columns that shouldn't be there)
unnamed_columns = [col for col in df_validated.columns if 'Unnamed' in col]
if unnamed_columns:
    print(f"Unexpected columns found: {unnamed_columns}")
else:
    print("No 'Unnamed' columns found. Validation passed for column names.")

# Validation 2: Check if 'Full_Address' exists and if it contains the combined address
if 'Full_Address' in df_validated.columns:
    print("'Full_Address' column exists. Validation passed for address combination.")
    # Sample a few rows to inspect the values
    print(df_validated[['Full_Address']].head())
else:
    print("'Full_Address' column is missing. Address combination validation failed.")

# Validation 3: Ensure that there are no duplicates
initial_row_count = len(df_validated)
df_no_duplicates = df_validated.drop_duplicates()
final_row_count = len(df_no_duplicates)

if initial_row_count == final_row_count:
    print(f"No duplicates found. Row count: {initial_row_count}")
else:
    print(f"Duplicates were found and removed. Reduced from {initial_row_count} to {final_row_count}")

# Validation 4: Check if the correct columns were retained
expected_columns = ['VIN', 'Name', 'Mobile_Phone', 'Email', 'Brand', 'Vehicle_Series',
                    'Vehicle_Model', 'Engine_Number', 'Full_Address']

# Columns that should be present in the final cleaned DataFrame
missing_columns = [col for col in expected_columns if col not in df_validated.columns]
extra_columns = [col for col in df_validated.columns if col not in expected_columns]

if missing_columns:
    print(f"Missing expected columns: {missing_columns}")
else:
    print("All expected columns are present.")

if extra_columns:
    print(f"Unexpected extra columns found: {extra_columns}")
else:
    print("No unexpected extra columns found.")

# Validation Summary
print("\nValidation Summary:")
print(f"Total rows: {initial_row_count}")
print(f"Total columns: {len(df_validated.columns)}")

import pandas as pd
import os

# Define the path of the original CSV file and the output directory
file_path = '/content/nationwide_pan1.csv'
output_dir = '/content/new_cleaned_chunks/'

# Create the output directory if it does not exist
os.makedirs(output_dir, exist_ok=True)

# Define the number of chunks
num_chunks = 4

# Read the original CSV file in chunks
chunksize = sum(1 for _ in open(file_path)) // num_chunks

# Chunk the CSV and save each chunk as a separate file
chunk_number = 0
for chunk in pd.read_csv(file_path, chunksize=chunksize):
    output_file_path = os.path.join(output_dir, f'nationwide_pan1_chunk_{chunk_number + 1}.csv')
    chunk.to_csv(output_file_path, index=False)
    print(f'Chunk {chunk_number + 1} saved to {output_file_path}')
    chunk_number += 1

import pandas as pd
import os
import glob

# Define the directory containing the chunks and the output file path
chunks_dir = '/content/new_cleaned_chunks/'
output_file = '/content/nationwide_cleaned_final.csv'

# List all chunk files in the directory
chunk_files = sorted(glob.glob(os.path.join(chunks_dir, 'nationwide_pan1_chunk_*.csv')))

# Initialize an empty list to hold dataframes
dataframes = []

# Read each chunk and append it to the list
for chunk_file in chunk_files:
    df_chunk = pd.read_csv(chunk_file)
    dataframes.append(df_chunk)

# Concatenate all the dataframes into one
final_df = pd.concat(dataframes, ignore_index=True)

# Save the final merged DataFrame to a new CSV file
final_df.to_csv(output_file, index=False)

print(f'Merged file saved as {output_file}')